{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d069a14",
   "metadata": {
    "id": "a63262ad"
   },
   "source": [
    "# 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c4de50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0 4.11.3 2.3.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "print(\n",
    "    datasets.__version__,\n",
    "transformers.__version__,\n",
    "torch.__version__)\n",
    "\n",
    "# 라이브러리 불러오기\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33fcb7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306b40e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "torch._C._cuda_getDeviceCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b509571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9788161",
   "metadata": {
    "id": "SlAOhRs09rt2"
   },
   "outputs": [],
   "source": [
    "# 한글 폰트에 문제가 생겼을 때\n",
    "\n",
    "# 한글 폰트 설치\n",
    "!apt-get update -qq\n",
    "!apt-get install -qq fonts-nanum\n",
    "\n",
    "# 설치한 폰트를 matplotlib에서 사용할 수 있도록 설정\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 나눔 폰트 경로 설정\n",
    "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
    "\n",
    "# 폰트 매니저에 폰트 추가\n",
    "fm.fontManager.addfont(font_path)\n",
    "plt.rc('font', family='NanumGothic')  # 폰트 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2c80f7",
   "metadata": {
    "id": "482d4066"
   },
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0210d59f",
   "metadata": {
    "id": "7ae4fc0d"
   },
   "source": [
    "#### 학습 데이터 불러오기\n",
    "\n",
    "프롬프트로 생성된 '일반 대화' 합성 데이터와 원본 데이터가 합쳐진 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c02387c",
   "metadata": {
    "id": "b13d6792"
   },
   "outputs": [],
   "source": [
    "train_data_path =\"../share/data/conversations.csv\"\n",
    "# 원본 데이터 저장\n",
    "origin_data = pd.read_csv(train_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822f0e89",
   "metadata": {},
   "source": [
    "https://huggingface.co/gogamza/kobart-base-v2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b97f3a38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ilpz-Cj86KD",
    "outputId": "b72bbfd4-de19-4fe5-d443-2142e78dc73f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "갈취 대화           981\n",
       "기타 괴롭힘 대화      1094\n",
       "일반 대화          1000\n",
       "직장 내 괴롭힘 대화     979\n",
       "협박 대화           896\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 클래스 별 대화 확인해보기\n",
    "class_counts = origin_data.groupby('class').size()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca27b4",
   "metadata": {
    "id": "a2dc1dd9"
   },
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3480c1d",
   "metadata": {
    "id": "01b2fd43"
   },
   "source": [
    "### 기본 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b585f4",
   "metadata": {
    "id": "5StEP_y9HGHC"
   },
   "source": [
    "#### 클래스 컬럼 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14485e65",
   "metadata": {
    "id": "GjX684X4HDdT"
   },
   "outputs": [],
   "source": [
    "# 'class'를 'type'으로 매핑하는 딕셔너리 생성하기\n",
    "class_to_type = {\n",
    "    '협박 대화': 0,\n",
    "    '갈취 대화': 1,\n",
    "    '직장 내 괴롭힘 대화': 2,\n",
    "    '기타 괴롭힘 대화': 3,\n",
    "    '일반 대화': 4\n",
    "}\n",
    "type_to_class = {val:key for key, val in class_to_type.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "724f1a21",
   "metadata": {
    "id": "xRYGA8ftH-4V"
   },
   "outputs": [],
   "source": [
    "# 'class' 열을 기반으로 새로운 'type' 열 추가하기\n",
    "origin_data['type'] = origin_data['class'].map(class_to_type)\n",
    "# 기존 idx, class 컬럼 삭제하기\n",
    "origin_data.drop(['idx', 'class'], axis=1, inplace=True)\n",
    "# new_train_data를 train_data에 덮어 씌우기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf515a2",
   "metadata": {
    "id": "62e949ca"
   },
   "source": [
    "### 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190c23d6",
   "metadata": {},
   "source": [
    "#### 한글 외 문자 삭제\n",
    "한글, '?', '!', '.', '.', 공백 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87dcbe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence): \n",
    "    # \\n을 공백으로 바꾸기\n",
    "    sentence = re.sub(\"\\n\", \" \", sentence)\n",
    "    \n",
    "    # (ㄱ-ㅎ, ㅏ-ㅣ, \".\", \"?\", \"!\", \",\", ' ')를 제외한 모든 문자를 없애기\n",
    "    sentence = re.sub(\"[^ㄱ-ㅣ가-힣.?!, ]\", \"\", sentence)\n",
    "    \n",
    "    # 단어와 구두점(punctuation) 사이에 공백 추가\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae766ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_data['preprocessed'] = origin_data['conversation'].map(preprocess_sentence)\n",
    "origin_data.drop('conversation', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20d9e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(origin_data, test_size=0.2, shuffle=True, random_state=42)\n",
    "val_data, test_data = train_test_split(val_data, test_size=0.5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bb43621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dataset objects\n",
    "train_ds = Dataset.from_pandas(origin_data, split=\"train\")#.remove_columns(['__index_level_0__'])\n",
    "val_ds = Dataset.from_pandas(val_data, split=\"val\")#.remove_columns(['__index_level_0__'])\n",
    "test_ds = Dataset.from_pandas(test_data, split=\"test\")#.remove_columns(['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df12d264",
   "metadata": {
    "id": "f2e6fa92"
   },
   "source": [
    "#### 토큰화 ( 정수 인코딩 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cd5da44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizerFast, BartForSequenceClassification\n",
    "\n",
    "tokenizer = BartTokenizerFast.from_pretrained('hyunwoongko/kobart')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bb9ce416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_sequence(sample):\n",
    "    text = sample[\"preprocessed\"]\n",
    "    label = sample[\"type\"]\n",
    "    encoded_sequence = tokenizer(text, truncation = True, padding = True, return_tensors='pt')\n",
    "    encoded_sequence[\"labels\"] = label\n",
    "#     encoded_sequence[\"input_sentence\"] = tokenizer.batch_decode(encoded_sequence.input_ids)\n",
    "    return encoded_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2aa6fa8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549c4bd535584535aa71599d0aa8331e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4950 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2791ee2bf24745b025c3cc9d3af528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/495 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b828d67a5604121ab3d6fb5bcb456e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/495 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_ds.map(create_input_sequence, batch_size = 1, remove_columns = [\"preprocessed\", \"type\"]).shuffle(seed=42)\n",
    "valid_dataset = val_ds.map(create_input_sequence, batch_size = 1, remove_columns = [\"preprocessed\", \"type\"]).shuffle(seed=42)\n",
    "test_dataset = test_ds.map(create_input_sequence, batch_size = 1, remove_columns = [\"preprocessed\", \"type\"]).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ffa69ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos token/id: <s> 0\n",
      "eos token/id: </s> 1\n",
      "sep_token/id: </s> 1\n",
      "additional_special_tokens: []\n"
     ]
    }
   ],
   "source": [
    "print('bos token/id: {} {}\\n'\n",
    "      'eos token/id: {} {}\\n'\n",
    "      'sep_token/id: {} {}\\n'\n",
    "      'additional_special_tokens: {}'.format(\n",
    "    tokenizer.bos_token, tokenizer.bos_token_id,\n",
    "    tokenizer.eos_token,tokenizer.eos_token_id,\n",
    "    tokenizer.sep_token, tokenizer.sep_token_id,\n",
    "    tokenizer.additional_special_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "48414071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hyunwoongko/kobart/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a06e1ed0b8182dfd1d23c5521c0e5a11ebcd2725cd41ac7f4d099ef7fd6dc12f.db3846b25a7f024766574201ac1e37d3fc3bb9a26f1893cd9aabc80ad2bf4067\n",
      "Model config BartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"\\ud611\\ubc15 \\ub300\\ud654\",\n",
      "    \"1\": \"\\uac08\\ucde8 \\ub300\\ud654\",\n",
      "    \"2\": \"\\uc9c1\\uc7a5 \\ub0b4 \\uad34\\ub86d\\ud798 \\ub300\\ud654\",\n",
      "    \"3\": \"\\uae30\\ud0c0 \\uad34\\ub86d\\ud798 \\ub300\\ud654\",\n",
      "    \"4\": \"\\uc77c\\ubc18 \\ub300\\ud654\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"\\uac08\\ucde8 \\ub300\\ud654\": 1,\n",
      "    \"\\uae30\\ud0c0 \\uad34\\ub86d\\ud798 \\ub300\\ud654\": 3,\n",
      "    \"\\uc77c\\ubc18 \\ub300\\ud654\": 4,\n",
      "    \"\\uc9c1\\uc7a5 \\ub0b4 \\uad34\\ub86d\\ud798 \\ub300\\ud654\": 2,\n",
      "    \"\\ud611\\ubc15 \\ub300\\ud654\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/hyunwoongko/kobart/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/654dc82731d78260169bd8567fe3ce5fce919dfa3661f40d99d9d6d3cc5be6e3.ef5977990801f5b7dbc37adda9fe5948ed9829c75ac20e99bf026098743b1978\n",
      "All model checkpoint weights were used when initializing BartForSequenceClassification.\n",
      "\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized: ['classification_head.dense.weight', 'classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForSequenceClassification, BartConfig\n",
    "default_model_config = BartConfig.from_pretrained('hyunwoongko/kobart', \n",
    "                                          num_labels=5, \n",
    "                                          label2id=class_to_type,\n",
    "                                          id2label=type_to_class,\n",
    "                                          bos_token_id=tokenizer.bos_token_id,\n",
    "                                          eos_token_id=tokenizer.eos_token_id,\n",
    "                                          pad_token_id=tokenizer.pad_token_id,\n",
    "                                         )\n",
    "model = BartForSequenceClassification.from_pretrained('hyunwoongko/kobart', config=default_model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d62866f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e6e5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93a7be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CLS_NUM = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bfb716",
   "metadata": {
    "id": "0b636ead"
   },
   "source": [
    "## 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a211b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\", \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs = 10,             # Total number of training epochs\n",
    "    per_device_train_batch_size = 24,  # Batch size per device during training\n",
    "    per_device_eval_batch_size = 64,   # Batch size for evaluation\n",
    "    warmup_steps = 500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay = 0.01,               # Strength of weight decay\n",
    ")\n",
    "\n",
    "# training_args = training_args.set_training(\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62f33fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "import numpy as np\n",
    "# import evaluate\n",
    "from datasets import load_metric\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "metric_f1 = load_metric(\"f1\")\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    metric_acc = load_metric(\"accuracy\")\n",
    "    metric_f1 = load_metric(\"f1\", trust_remote_code=True)\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis = 1)\n",
    "    result = {}\n",
    "    result[\"accuracy\"] = metric_acc.compute(predictions = preds, references = p.label_ids)[\"accuracy\"]\n",
    "    result[\"f1\"] = metric_f1.compute(predictions = preds, references = p.label_ids, average = 'macro')[\"f1\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ad08c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb==0.16.0 -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3398ed0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=DLthon_finetune_koBart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhojae-choi\u001b[0m (\u001b[33maiffel_minions\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_PROJECT=DLthon_finetune_koBart\n",
    "os.environ['TOKENIZERS_PARALLELISM']='true'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
    "os.environ['WANDB_API_KEY'] = '746fb761ab2f1b53db2dafef7340caad69224513'\n",
    "import wandb\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# from wandb.keras import WandbCallback\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c45ef711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.11.3'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "686c9df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/hyunwoongko/kobart/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a06e1ed0b8182dfd1d23c5521c0e5a11ebcd2725cd41ac7f4d099ef7fd6dc12f.db3846b25a7f024766574201ac1e37d3fc3bb9a26f1893cd9aabc80ad2bf4067\n",
      "Model config BartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"\\ud611\\ubc15 \\ub300\\ud654\",\n",
      "    \"1\": \"\\uac08\\ucde8 \\ub300\\ud654\",\n",
      "    \"2\": \"\\uc9c1\\uc7a5 \\ub0b4 \\uad34\\ub86d\\ud798 \\ub300\\ud654\",\n",
      "    \"3\": \"\\uae30\\ud0c0 \\uad34\\ub86d\\ud798 \\ub300\\ud654\",\n",
      "    \"4\": \"\\uc77c\\ubc18 \\ub300\\ud654\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"\\uac08\\ucde8 \\ub300\\ud654\": 1,\n",
      "    \"\\uae30\\ud0c0 \\uad34\\ub86d\\ud798 \\ub300\\ud654\": 3,\n",
      "    \"\\uc77c\\ubc18 \\ub300\\ud654\": 4,\n",
      "    \"\\uc9c1\\uc7a5 \\ub0b4 \\uad34\\ub86d\\ud798 \\ub300\\ud654\": 2,\n",
      "    \"\\ud611\\ubc15 \\ub300\\ud654\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_training_args = {\n",
    "    'output_dir':\"test_trainer\", \n",
    "    'evaluation_strategy':\"steps\",\n",
    "    'report_to':\"wandb\",\n",
    "    'logging_steps':20,\n",
    "    'load_best_model_at_end':True,\n",
    "    'num_train_epochs': 10,             # Total number of training epochs\n",
    "    'per_device_train_batch_size': 24,  # Batch size per device during training\n",
    "    'per_device_eval_batch_size': 64,   # Batch size for evaluation\n",
    "    'warmup_steps': 500,                # Number of warmup steps for learning rate scheduler\n",
    "    'weight_decay': 0.01,               # Strength of weight decay\n",
    "}\n",
    "\n",
    "default_model_config = BartConfig.from_pretrained(\n",
    "    'hyunwoongko/kobart', \n",
    "    num_labels=5, \n",
    "    label2id=class_to_type,\n",
    "    id2label=type_to_class,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "def separate_config(wandb_config):\n",
    "    model_config = BartConfig(default_model_config)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        **default_training_args  \n",
    "    )\n",
    "    training_args.num_train_epochs = wandb_config.num_epochs\n",
    "    \n",
    "    return model_config, training_args\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a44fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_wandb_config = {\n",
    "#         \"vocab\" : VOCAB_SIZE,\n",
    "#         \"embeddings\" : 128,\n",
    "#         \"units_128\" : 128,\n",
    "#         \"units_256\" : 256,\n",
    "#         \"units_512\" : 512,\n",
    "#         \"units_1024\" : 1024,\n",
    "#         \"units_2048\" : 2048,\n",
    "#         \"kernel_3\" : 3,\n",
    "#         \"kernel_5\" : 5,\n",
    "        \"loss\" : \"sparse_categorical_crossentropy\",\n",
    "        \"num_epochs\": 10,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd02a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_config):\n",
    "    \n",
    "    model = BartForSequenceClassification.from_pretrained(\n",
    "        'hyunwoongko/kobart').to('cuda')\n",
    "    \n",
    "    for w in model.model.parameters():\n",
    "        w._trainable = False\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5489043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.integrations import WandbCallback\n",
    "def train():\n",
    "    wandb.init(config = default_wandb_config)\n",
    "    wandb_config = wandb.config\n",
    "    model_config, training_args = separate_config(wandb_config)\n",
    "    \n",
    "    \n",
    "    model = build_model(model_config)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    trainer.add_callback(WandbCallback())\n",
    "    \n",
    "    \n",
    "    trainer.train()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "82d34489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9ofojeh7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.012 MB uploaded\\r'), FloatProgress(value=0.12417700199050681, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twilight-sky-18</strong> at: <a href='https://wandb.ai/aiffel_minions/DLthon_finetune_koBart/runs/9ofojeh7' target=\"_blank\">https://wandb.ai/aiffel_minions/DLthon_finetune_koBart/runs/9ofojeh7</a><br/> View job at <a href='https://wandb.ai/aiffel_minions/DLthon_finetune_koBart/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjIzNDY3NzcxMA==/version_details/v9' target=\"_blank\">https://wandb.ai/aiffel_minions/DLthon_finetune_koBart/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjIzNDY3NzcxMA==/version_details/v9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240626_124647-9ofojeh7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9ofojeh7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/dlthon-minions/dlthon-stuart/wandb/run-20240626_125658-74hwp2h6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aiffel_minions/DLthon_finetune_koBart/runs/74hwp2h6' target=\"_blank\">comfy-durian-19</a></strong> to <a href='https://wandb.ai/aiffel_minions/DLthon_finetune_koBart' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aiffel_minions/DLthon_finetune_koBart' target=\"_blank\">https://wandb.ai/aiffel_minions/DLthon_finetune_koBart</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aiffel_minions/DLthon_finetune_koBart/runs/74hwp2h6' target=\"_blank\">https://wandb.ai/aiffel_minions/DLthon_finetune_koBart/runs/74hwp2h6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 20\n",
      "PyTorch: setting up devices\n",
      "loading configuration file https://huggingface.co/hyunwoongko/kobart/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a06e1ed0b8182dfd1d23c5521c0e5a11ebcd2725cd41ac7f4d099ef7fd6dc12f.db3846b25a7f024766574201ac1e37d3fc3bb9a26f1893cd9aabc80ad2bf4067\n",
      "Model config BartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/hyunwoongko/kobart/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/654dc82731d78260169bd8567fe3ce5fce919dfa3661f40d99d9d6d3cc5be6e3.ef5977990801f5b7dbc37adda9fe5948ed9829c75ac20e99bf026098743b1978\n",
      "All model checkpoint weights were used when initializing BartForSequenceClassification.\n",
      "\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at hyunwoongko/kobart and are newly initialized: ['classification_head.dense.weight', 'classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are adding a <class 'transformers.integrations.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "WandbCallback\n",
      "NotebookProgressCallback\n",
      "***** Running training *****\n",
      "  Num examples = 4950\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 24\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2070\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 78 at dim 2 (got 120)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_661/3309375915.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"WANDB_PROJECT\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"DLthon_finetune_koBart\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_661/1874510240.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         batch = self.tokenizer.pad(\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   2793\u001b[0m                 \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2795\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m     def create_token_type_ids_from_sequences(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    719\u001b[0m                         \u001b[0;34m\"Please see if a fast version of this tokenizer is available to have this feature available.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                     )\n\u001b[0;32m--> 721\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    722\u001b[0m                     \u001b[0;34m\"Unable to create tensor, you should probably activate truncation and/or padding \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0;34m\"with 'padding=True' 'truncation=True' to have batched tensors with the same length.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" \n",
    "os.environ[\"WANDB_PROJECT\"] = \"DLthon_finetune_koBart\"\n",
    "# torch.cuda.empty_cache() \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env WANDB_PROJECT=DLthon_finetune_koBart\n",
    "\n",
    "sweep_config = {\n",
    "    \"name\": \"sweep_kobart_nlp\",\n",
    "    \"metric\": {\"name\": \"val_loss\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\" : {\n",
    "            \"min\" : 0.001,\n",
    "            \"max\" : 0.01\n",
    "            },\n",
    "        \"num_epochs\" : {\n",
    "            \"distribution\" : \"int_uniform\",\n",
    "            \"min\" : 5,\n",
    "            \"max\" : 10\n",
    "            }\n",
    "                    \n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config,\n",
    "                       entity = 'aiffel_minions',\n",
    "                       project = 'DLthon_finetune_koBart',\n",
    "                      )\n",
    "# run the sweep\n",
    "wandb.agent(sweep_id,\n",
    "            function=train,\n",
    "            count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0755f2",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85ce336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.login(key = '746fb761ab2f1b53db2dafef7340caad69224513')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e229011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"name\": \"sweep_test_nlp\",\n",
    "    \"metric\": {\"name\": \"val_loss\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\" : {\n",
    "            \"min\" : 0.001,\n",
    "            \"max\" : 0.1\n",
    "            },\n",
    "        \"epoch\" : {\n",
    "            \"distribution\" : \"int_uniform\",\n",
    "            \"min\" : 5,\n",
    "            \"max\" : 10\n",
    "            }\n",
    "                    \n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e286ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tensorflow import keras\n",
    "def build_model(config):\n",
    "    model=keras.models.Sequential()\n",
    "    model.add(keras.layers.Embedding(config.vocab, config.embeddings))\n",
    "    model.add(keras.layers.GRU(units = config.units_1, return_sequences = True))\n",
    "    model.add(keras.layers.GRU(units = config.units_2))\n",
    "    model.add(keras.layers.Dense(config.units_3, activation='relu'))\n",
    "    model.add(keras.layers.Dense(config.class_num, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Config, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    confusion_matrix, \n",
    "    ConfusionMatrixDisplay, \n",
    "    classification_report\n",
    "    )\n",
    "\n",
    "\n",
    "def train():\n",
    "    default_config = {\n",
    "        \"vocab\" : VOCAB_SIZE,\n",
    "        \"embeddings\" : 64,\n",
    "        \"units_1\" : 256,\n",
    "        \"units_2\" : 256,\n",
    "        \"units_3\" : 1024,\n",
    "        \"class_num\" : CLS_NUM,\n",
    "        \"learning_rate\" : 0.005,\n",
    "        \"optimizer\" : \"adam\",\n",
    "        \"loss\" : \"sparse_categorical_crossentropy\",\n",
    "        \"metrics\" : [\"accuracy\"],\n",
    "        \"epoch\" : 10,\n",
    "        \"batch_size\" : 32\n",
    "    }\n",
    "\n",
    "    wandb.init(config = default_config)\n",
    "    config = wandb.config\n",
    "#     config = Config(default_config)\n",
    "\n",
    "    # Model\n",
    "    keras.backend.clear_session()\n",
    "    model = build_model(config)\n",
    "\n",
    "    # 머신 러닝 학습때 여러가지 optimzier를 사용할 경우나 learning rate를 조절할 경우에는 아래와 같은 형태의 코드를 응용합니다.\n",
    "    if config.optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate = config.learning_rate)\n",
    "    \n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss = config.loss,\n",
    "                  metrics = config.metrics)\n",
    "\n",
    "    # WandbCallback 함수는 후술합니다.\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "              epochs = config.epoch,\n",
    "              shuffle=True,\n",
    "              batch_size = config.batch_size,\n",
    "              validation_data = (X_val, y_val),\n",
    "              callbacks = [WandbCallback()],\n",
    "             )\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "    pred_test = model.predict(X_test).argmax(axis=1)\n",
    "    f1_score_res = f1_score(y_test, pred_test, average='micro')\n",
    "    # wandb.log 함수 안에 기록하고 싶은 정보를 담습니다.\n",
    "\n",
    "    wandb.log({\"Test Accuracy Rate: \" : round(test_accuracy * 100, 2),\n",
    "               \"Test F1 Score: \": round(f1_score_res * 100, 2),\n",
    "               \"Test Error Rate: \" : round((1 - test_accuracy) * 100, 2)})\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity와 project에 본인의 아이디와 프로젝트명을 입력하세요\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config,\n",
    "                       entity = 'aiffel_minions',\n",
    "                       project = 'DLthon_baseline_GRU',\n",
    "                      )\n",
    "\n",
    "# run the sweep\n",
    "wandb.agent(sweep_id,\n",
    "            function=train,\n",
    "            count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61178272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc43aa8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
