{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e66971",
   "metadata": {
    "id": "a63262ad"
   },
   "source": [
    "# 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1094983d",
   "metadata": {
    "id": "cf7ba9f6"
   },
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import sentencepiece as spm\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Mecab\n",
    "import csv \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a21aee",
   "metadata": {
    "id": "SlAOhRs09rt2"
   },
   "outputs": [],
   "source": [
    "# 한글 폰트에 문제가 생겼을 때\n",
    "\n",
    "# 한글 폰트 설치\n",
    "!apt-get update -qq\n",
    "!apt-get install -qq fonts-nanum\n",
    "\n",
    "# 설치한 폰트를 matplotlib에서 사용할 수 있도록 설정\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 나눔 폰트 경로 설정\n",
    "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
    "\n",
    "# 폰트 매니저에 폰트 추가\n",
    "fm.fontManager.addfont(font_path)\n",
    "plt.rc('font', family='NanumGothic')  # 폰트 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f3308",
   "metadata": {
    "id": "482d4066"
   },
   "source": [
    "## 데이터 업로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399bfb21",
   "metadata": {
    "id": "7ae4fc0d"
   },
   "source": [
    "#### 학습 데이터 불러오기\n",
    "\n",
    "프롬프트로 생성된 '일반 대화' 합성 데이터와 원본 데이터가 합쳐진 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e38198",
   "metadata": {
    "id": "b13d6792"
   },
   "outputs": [],
   "source": [
    "train_data_path =\"../data/conversations.csv\"\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "# 원본 데이터 저장\n",
    "origin_data = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf90ede",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "960aefbc",
    "outputId": "0a63c37c-ea2e-465c-830f-f36405289505"
   },
   "outputs": [],
   "source": [
    "train_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd176093",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "662ed8cb",
    "outputId": "bf7af049-16d1-411f-e51b-7e2388e895c9"
   },
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8986ada9",
   "metadata": {
    "id": "daad0be9"
   },
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f070d4",
   "metadata": {
    "id": "a2dc1dd9"
   },
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde1ace",
   "metadata": {
    "id": "01b2fd43"
   },
   "source": [
    "### 기본 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249e0de6",
   "metadata": {
    "id": "OYxUovemAv20"
   },
   "source": [
    "#### 중복값 여부 찾아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e2e9c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "cB1L0F_TAi_C",
    "outputId": "b61694e1-9aab-493d-92b0-2d51a5c1a8a8"
   },
   "outputs": [],
   "source": [
    "# 중복값 찾기\n",
    "duplicates = train_data[train_data.duplicated()]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7dbb21",
   "metadata": {},
   "source": [
    "중복값 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ac8df",
   "metadata": {
    "id": "j0pOinZUARe5"
   },
   "source": [
    "#### 결측치 여부 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063bc7a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5JBxYdiAMVC",
    "outputId": "e600fe25-72c4-47c4-e687-cacd5a91e6f9"
   },
   "outputs": [],
   "source": [
    "# 결측치 여부 확인하기\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c534724",
   "metadata": {},
   "source": [
    "결측치 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c85c6",
   "metadata": {
    "id": "5StEP_y9HGHC"
   },
   "source": [
    "#### 클래스 컬럼 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebbe80b",
   "metadata": {
    "id": "GjX684X4HDdT"
   },
   "outputs": [],
   "source": [
    "# 'class'를 'type'으로 매핑하는 딕셔너리 생성하기\n",
    "class_to_type = {\n",
    "    '협박 대화': 0,\n",
    "    '갈취 대화': 1,\n",
    "    '직장 내 괴롭힘 대화': 2,\n",
    "    '기타 괴롭힘 대화': 3,\n",
    "    '일반 대화': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04986cd9",
   "metadata": {
    "id": "xRYGA8ftH-4V"
   },
   "outputs": [],
   "source": [
    "# 'class' 열을 기반으로 새로운 'type' 열 추가하기\n",
    "train_data['type'] = train_data['class'].map(class_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb7209e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "EtcFaPyJIHGk",
    "outputId": "d67bb466-6612-4dd5-e289-4031489f5299"
   },
   "outputs": [],
   "source": [
    "# type 열 추가했는지 확인하기\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06d9d6",
   "metadata": {
    "id": "A1jo-6qkHstg"
   },
   "outputs": [],
   "source": [
    "# 기존 idx, class 컬럼 삭제하기\n",
    "new_train_data = train_data.drop(['idx', 'class'], axis=1)\n",
    "# idx, class 컬럼 삭제했는지 확인하기\n",
    "new_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0883466",
   "metadata": {
    "id": "bPud0R2yIasN"
   },
   "outputs": [],
   "source": [
    "# new_train_data를 train_data에 덮어 씌우기\n",
    "train_data = new_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ebd8f6",
   "metadata": {
    "id": "62e949ca"
   },
   "source": [
    "### 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da391d",
   "metadata": {},
   "source": [
    "#### 한글 외 문자 삭제\n",
    "한글, '?', '!', '.', '.', 공백 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8292e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence): \n",
    "    # \\n을 공백으로 바꾸기\n",
    "    sentence = re.sub(\"\\n\", \" \", sentence)\n",
    "    \n",
    "    # (ㄱ-ㅎ, ㅏ-ㅣ, \".\", \"?\", \"!\", \",\", ' ')를 제외한 모든 문자를 없애기\n",
    "    sentence = re.sub(\"[^ㄱ-ㅣ가-힣.?!, ]\", \"\", sentence)\n",
    "    \n",
    "    # 단어와 구두점(punctuation) 사이에 공백 추가\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d0545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 데이터 새로운 column에 저장\n",
    "train_data['preprocessed'] = train_data['conversation'].apply(preprocess_sentence)\n",
    "train_data['preprocessed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c304dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 'conversation' column 전처리한 데이터로 바꾸기\n",
    "train_data['conversation'] = train_data['preprocessed']\n",
    "train_data.drop('preprocessed', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061ed3c8",
   "metadata": {},
   "source": [
    "#### 불용어 삭제\n",
    "[불용어 리스트 출처](https://www.ranks.nl/stopwords/korean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 리스트 불러오기\n",
    "stopwords_path = \"ko_stopwords.txt\"\n",
    "with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a9e0d",
   "metadata": {
    "id": "f2e6fa92"
   },
   "source": [
    "#### 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a37e07",
   "metadata": {},
   "source": [
    "```!pip install sentencepiece```  \n",
    "```!pip install konlpy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c17d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전 크기 20000으로 제한\n",
    "VOCAB_SIZE=20000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2236fca1",
   "metadata": {},
   "source": [
    "토크나이저 선정\n",
    "1. SentencePiece  \n",
    "[ref](https://wikidocs.net/86657)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49504917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentencePiece의 입력으로 사용하기 위해서 데이터를 txt 파일로 저장\n",
    "with open('conversations.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(train_data['conversation']))\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input=conversations.txt ' + \n",
    "    '--model_prefix=dktc ' +\n",
    "    f'--vocab_size={VOCAB_SIZE} ' +\n",
    "    '--model_type=bpe ' + \n",
    "    '--pad_id=0 --pad_piece=<pad> ' +\n",
    "    '--unk_id=1 --unk_piece=<unk> ' +\n",
    "    '--bos_id=2 --bos_piece=<sos> ' +\n",
    "    '--eos_id=3 --eos_piece=<eos> ' + \n",
    "    '--max_sentence_length=9999'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_df = pd.read_csv('dktc.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "word_index_df.columns=['words', 'idx']\n",
    "word_index_df['idx'] = np.arrange(len(word_index_df))\n",
    "word_to_index = word_index_df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fed229",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d525c41",
   "metadata": {},
   "source": [
    "2. konlpy  \n",
    "1) Okt  \n",
    "2) Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okt로 토큰화\n",
    "tokenizer=Okt()\n",
    "#tokenizer=Mecab()\n",
    "\n",
    "def tokenize(conversation, tokenizer):\n",
    "    return [token for token in tokenizer.morphs(conversation) if token not in stopwords]\n",
    "\n",
    "# 각 conversation을 토큰화하여 새로운 열 'tokenized'에 저장\n",
    "train_data['tokenized'] = train_data['conversation'].apply(lambda x: tokenize(x, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e18c8",
   "metadata": {},
   "source": [
    "#### 시작 토큰, 종료 토큰 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc38fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "\n",
    "# 시작 토큰과 종료 토큰을 추가하는 함수 정의\n",
    "def add_special_tokens(tokens):\n",
    "    return [special_tokens[2]] + tokens + [special_tokens[3]]\n",
    "\n",
    "# 'tokenized' 열에 함수 적용하여 시작 토큰과 종료 토큰 추가\n",
    "train_data['tokenized'] = train_data['tokenized'].apply(add_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a127f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화된 단어 확인\n",
    "train_data['tokenized']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6cdb5",
   "metadata": {},
   "source": [
    "#### 단어사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75620fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'tokenized' 열의 모든 토큰을 모아 등장 빈도 계산\n",
    "all_tokens = [token for tokens in train_data['tokenized'] for token in tokens]\n",
    "counter = Counter(all_tokens)\n",
    "# 가장 많이 등장하는 단어 vocab_size 개수만큼 남기기\n",
    "counter = counter.most_common(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306731d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary 파일로 저장\n",
    "word_to_index_path = './vocab.txt'\n",
    "\n",
    "def save_word_index(counter):\n",
    "    with open(word_to_index_path, 'w') as f:\n",
    "        for idx, (word, _) in enumerate(counter):\n",
    "            line = f\"{word}: {idx}\\n\"\n",
    "            f.write(line)\n",
    "            \n",
    "#save_word_index(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca87312",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {token: idx for idx, token in enumerate(special_tokens[:2])}\n",
    "word_to_index.update({token: idx + len(special_tokens[:2]) for idx, (token, _) in enumerate(counter)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a868f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12777ad1",
   "metadata": {},
   "source": [
    "#### 정수인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85772b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'tokenized' 열의 데이터를 정수 인코딩\n",
    "def encode_tokens(tokens, word_to_index):\n",
    "    unk_index = word_to_index['<unk>']\n",
    "    return [word_to_index.get(token, unk_index) for token in tokens]\n",
    "\n",
    "train_data['encoded'] = train_data['tokenized'].apply(lambda x: encode_tokens(x, word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dffd3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62fe460",
   "metadata": {
    "id": "e970c394"
   },
   "source": [
    "## 전처리 후 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faf84a6",
   "metadata": {},
   "source": [
    "#### 토큰화된 데이터 길이 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895387ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'encoded' 열의 각 리스트의 길이 계산\n",
    "lengths = train_data['encoded'].apply(len)\n",
    "\n",
    "print(f\"최대 길이: {lengths.max()}\")\n",
    "print(f\"최소 길이: {lengths.min()}\")\n",
    "print(f\"평균 길이: {lengths.mean()}\")\n",
    "print(f\"표준편차: {lengths.std()}\")\n",
    "\n",
    "# 길이 분포 시각화\n",
    "plt.hist(lengths, bins=50, color='skyblue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ea1a8",
   "metadata": {},
   "source": [
    "#### 패딩 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f7c7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화 최대 길이 150으로 설정\n",
    "MAX_LENGTH = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550665ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['encoded'] = pad_sequences(train_data['encoded'], maxlen=MAX_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16de22b",
   "metadata": {
    "id": "0b636ead"
   },
   "source": [
    "## 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5267d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data['encoded']\n",
    "y = train_data['type']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(X_train))\n",
    "print('훈련 레이블의 개수 :', len(y_train))\n",
    "print('검증 데이터의 개수 :', len(X_val))\n",
    "print('검증 레이블의 개수 :', len(y_val))\n",
    "print('테스트 데이터의 개수 :', len(X_test))\n",
    "print('테스트 레이블의 개수 :', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67865163",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5874099b",
   "metadata": {},
   "source": [
    "```!pip install wandb==0.16.0```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ffff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.login(key = '809618c39f10bc0019fd6fd710cb28c698c30197')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39701c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"name\": \"sweep_test_nlp\",\n",
    "    \"metric\": {\"name\": \"val_loss\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\" : {\n",
    "            \"min\" : 0.001,\n",
    "            \"max\" : 0.1\n",
    "            },\n",
    "        \"epoch\" : {\n",
    "            \"distribution\" : \"int_uniform\",\n",
    "            \"min\" : 5,\n",
    "            \"max\" : 10\n",
    "            }\n",
    "                    \n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286ef24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train():\n",
    "    default_config = {\n",
    "        \"vocab\" : 20000,\n",
    "        \"embeddings\" : 64,\n",
    "        \"units_1\" : 256,\n",
    "        \"units_2\" : 256,\n",
    "        \"units_3\" : 1024,\n",
    "        \"class_num\" : 5,\n",
    "        \"learning_rate\" : 0.005,\n",
    "        \"optimizer\" : \"adam\",\n",
    "        \"loss\" : \"sparse_categorical_crossentropy\",\n",
    "        \"metrics\" : [\"accuracy\"],\n",
    "        \"epoch\" : 5,\n",
    "        \"batch_size\" : 32\n",
    "    }\n",
    "\n",
    "    wandb.init(config = default_config)\n",
    "    config = wandb.config\n",
    "\n",
    "    # Model\n",
    "\n",
    "    model=keras.models.Sequential()\n",
    "    model.add(keras.layers.Embedding(config.vocab, config.embeddings))\n",
    "    model.add(keras.layers.GRU(units = config.units_1, return_sequences = True))\n",
    "    model.add(keras.layers.GRU(units = config.units_2))\n",
    "    model.add(keras.layers.Dense(config.units_3, activation='relu'))\n",
    "    model.add(keras.layers.Dense(config.class_num, activation='softmax'))\n",
    "\n",
    "    # 머신 러닝 학습때 여러가지 optimzier를 사용할 경우나 learning rate를 조절할 경우에는 아래와 같은 형태의 코드를 응용합니다.\n",
    "\n",
    "    if config.optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate = config.learning_rate)\n",
    "    \n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss = config.loss,\n",
    "                  metrics = config.metrics)\n",
    "\n",
    "    # WandbCallback 함수는 후술합니다.\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "              epochs = config.epoch,\n",
    "              batch_size = config.batch_size,\n",
    "              validation_data = (X_val, y_val),\n",
    "              callbacks = [WandbCallback()])\n",
    "    \n",
    "    test_loss, test_accuracy, test_fs = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = tf.argmax(y_pred, axis=1)\n",
    "    test_f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "    \n",
    "    # wandb.log 함수 안에 기록하고 싶은 정보를 담습니다.\n",
    "    \n",
    "    wandb.log({\"Test Accuracy Rate: \" : round(test_accuracy * 100, 2),\n",
    "               \"Test F1 Score\": round(test_f1 * 100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b8860",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# entity와 project에 본인의 아이디와 프로젝트명을 입력하세요\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config,\n",
    "                       entity = '4-rldur0',\n",
    "                       project = 'test')\n",
    "\n",
    "# run the sweep\n",
    "wandb.agent(sweep_id,\n",
    "            function=train,\n",
    "            count=10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
