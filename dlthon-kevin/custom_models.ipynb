{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cecd03cf",
   "metadata": {
    "id": "a63262ad"
   },
   "source": [
    "# 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c9c21",
   "metadata": {
    "id": "cf7ba9f6"
   },
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import sentencepiece as spm\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Mecab\n",
    "import csv \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow_addons as tfa\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd8163",
   "metadata": {
    "id": "SlAOhRs09rt2"
   },
   "outputs": [],
   "source": [
    "# 한글 폰트에 문제가 생겼을 때\n",
    "\n",
    "# 한글 폰트 설치\n",
    "!apt-get update -qq\n",
    "!apt-get install -qq fonts-nanum\n",
    "\n",
    "# 설치한 폰트를 matplotlib에서 사용할 수 있도록 설정\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 나눔 폰트 경로 설정\n",
    "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
    "\n",
    "# 폰트 매니저에 폰트 추가\n",
    "fm.fontManager.addfont(font_path)\n",
    "plt.rc('font', family='NanumGothic')  # 폰트 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf02fe",
   "metadata": {
    "id": "482d4066"
   },
   "source": [
    "## 데이터 업로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c630f",
   "metadata": {
    "id": "7ae4fc0d"
   },
   "source": [
    "#### 학습 데이터 불러오기\n",
    "\n",
    "프롬프트로 생성된 '일반 대화' 합성 데이터와 원본 데이터가 합쳐진 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab541d0",
   "metadata": {
    "id": "b13d6792"
   },
   "outputs": [],
   "source": [
    "train_data_path =\"/aiffel/aiffel/dlthon-minions/share/data/conversations.csv\"\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "# 원본 데이터 저장\n",
    "origin_data = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239bf304",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "960aefbc",
    "outputId": "0a63c37c-ea2e-465c-830f-f36405289505"
   },
   "outputs": [],
   "source": [
    "train_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feadfba4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "662ed8cb",
    "outputId": "bf7af049-16d1-411f-e51b-7e2388e895c9"
   },
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9e3fc",
   "metadata": {
    "id": "daad0be9"
   },
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97519ca",
   "metadata": {
    "id": "a2dc1dd9"
   },
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c153b933",
   "metadata": {
    "id": "01b2fd43"
   },
   "source": [
    "### 기본 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26ad6d",
   "metadata": {
    "id": "OYxUovemAv20"
   },
   "source": [
    "#### 중복값 여부 찾아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c7f71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "cB1L0F_TAi_C",
    "outputId": "b61694e1-9aab-493d-92b0-2d51a5c1a8a8"
   },
   "outputs": [],
   "source": [
    "# 중복값 찾기\n",
    "duplicates = train_data[train_data.duplicated()]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58602b63",
   "metadata": {},
   "source": [
    "중복값 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9fde6",
   "metadata": {
    "id": "j0pOinZUARe5"
   },
   "source": [
    "#### 결측치 여부 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13057725",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5JBxYdiAMVC",
    "outputId": "e600fe25-72c4-47c4-e687-cacd5a91e6f9"
   },
   "outputs": [],
   "source": [
    "# 결측치 여부 확인하기\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9478ff",
   "metadata": {},
   "source": [
    "결측치 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c0b7be",
   "metadata": {
    "id": "5StEP_y9HGHC"
   },
   "source": [
    "#### 클래스 컬럼 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac2560c",
   "metadata": {
    "id": "GjX684X4HDdT"
   },
   "outputs": [],
   "source": [
    "# 'class'를 'type'으로 매핑하는 딕셔너리 생성하기\n",
    "class_to_type = {\n",
    "    '협박 대화': 0,\n",
    "    '갈취 대화': 1,\n",
    "    '직장 내 괴롭힘 대화': 2,\n",
    "    '기타 괴롭힘 대화': 3,\n",
    "    '일반 대화': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6d1ef",
   "metadata": {
    "id": "xRYGA8ftH-4V"
   },
   "outputs": [],
   "source": [
    "# 'class' 열을 기반으로 새로운 'type' 열 추가하기\n",
    "train_data['type'] = train_data['class'].map(class_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c2c620",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "EtcFaPyJIHGk",
    "outputId": "d67bb466-6612-4dd5-e289-4031489f5299"
   },
   "outputs": [],
   "source": [
    "# type 열 추가했는지 확인하기\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2bc9e",
   "metadata": {
    "id": "A1jo-6qkHstg"
   },
   "outputs": [],
   "source": [
    "# 기존 idx, class 컬럼 삭제하기\n",
    "new_train_data = train_data.drop(['idx', 'class'], axis=1)\n",
    "# idx, class 컬럼 삭제했는지 확인하기\n",
    "new_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449a654",
   "metadata": {
    "id": "bPud0R2yIasN"
   },
   "outputs": [],
   "source": [
    "# new_train_data를 train_data에 덮어 씌우기\n",
    "train_data = new_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb6a9d7",
   "metadata": {
    "id": "62e949ca"
   },
   "source": [
    "### 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b588106",
   "metadata": {},
   "source": [
    "#### 한글 외 문자 삭제\n",
    "한글, '?', '!', '.', '.', 공백 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c020d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence): \n",
    "    # \\n을 공백으로 바꾸기\n",
    "    sentence = re.sub(\"\\n\", \" \", sentence)\n",
    "    \n",
    "    # (ㄱ-ㅎ, ㅏ-ㅣ, \".\", \"?\", \"!\", \",\", ' ')를 제외한 모든 문자를 없애기\n",
    "    sentence = re.sub(\"[^ㄱ-ㅣ가-힣.?!, ]\", \"\", sentence)\n",
    "    \n",
    "    # 단어와 구두점(punctuation) 사이에 공백 추가\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f7d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 데이터 새로운 column에 저장\n",
    "train_data['preprocessed'] = train_data['conversation'].apply(preprocess_sentence)\n",
    "train_data['preprocessed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac86ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 'conversation' column 전처리한 데이터로 바꾸기\n",
    "train_data['conversation'] = train_data['preprocessed']\n",
    "train_data.drop('preprocessed', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faf40af",
   "metadata": {},
   "source": [
    "#### 불용어 삭제\n",
    "[불용어 리스트 출처](https://www.ranks.nl/stopwords/korean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f73143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 리스트 불러오기\n",
    "stopwords_path = \"ko_stopwords.txt\"\n",
    "with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f64e593",
   "metadata": {
    "id": "f2e6fa92"
   },
   "source": [
    "#### 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45abd1d",
   "metadata": {},
   "source": [
    "```!pip install sentencepiece```  \n",
    "```!pip install konlpy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c51a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전 크기 20000으로 제한\n",
    "VOCAB_SIZE=20000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1a371",
   "metadata": {},
   "source": [
    "토크나이저 선정\n",
    "1. SentencePiece  \n",
    "[ref](https://wikidocs.net/86657)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3577583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# sentencePiece의 입력으로 사용하기 위해서 데이터를 txt 파일로 저장\n",
    "with open('conversations.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(train_data['conversation']))\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input=conversations.txt ' + \n",
    "    '--model_prefix=dktc ' +\n",
    "    f'--vocab_size={VOCAB_SIZE} ' +\n",
    "    '--model_type=bpe ' + \n",
    "    '--pad_id=0 --pad_piece=<pad> ' +\n",
    "    '--unk_id=1 --unk_piece=<unk> ' +\n",
    "    '--bos_id=2 --bos_piece=<sos> ' +\n",
    "    '--eos_id=3 --eos_piece=<eos> ' + \n",
    "    '--max_sentence_length=9999'\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54834af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''word_index_df = pd.read_csv('dktc.vocab', sep='\\t', header=None, quoting=csv.QUOTE_NONE)\n",
    "word_index_df.columns=['words', 'idx']\n",
    "word_index_df['idx'] = np.arrange(len(word_index_df))\n",
    "word_to_index = word_index_df.to_dict()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c847b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''word_to_index'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64dd01",
   "metadata": {},
   "source": [
    "2. konlpy  \n",
    "1) Okt  \n",
    "2) Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2195f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okt로 토큰화\n",
    "tokenizer=Okt()\n",
    "#tokenizer=Mecab()\n",
    "\n",
    "def tokenize(conversation, tokenizer):\n",
    "    return [token for token in tokenizer.morphs(conversation) if token not in stopwords]\n",
    "\n",
    "# 각 conversation을 토큰화하여 새로운 열 'tokenized'에 저장\n",
    "train_data['tokenized'] = train_data['conversation'].apply(lambda x: tokenize(x, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab0d75",
   "metadata": {},
   "source": [
    "#### 시작 토큰, 종료 토큰 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01147932",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "\n",
    "# 시작 토큰과 종료 토큰을 추가하는 함수 정의\n",
    "def add_special_tokens(tokens):\n",
    "    return [special_tokens[2]] + tokens + [special_tokens[3]]\n",
    "\n",
    "# 'tokenized' 열에 함수 적용하여 시작 토큰과 종료 토큰 추가\n",
    "train_data['tokenized'] = train_data['tokenized'].apply(add_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0145679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화된 단어 확인\n",
    "train_data['tokenized']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5941aec8",
   "metadata": {},
   "source": [
    "#### 단어사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c30e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'tokenized' 열의 모든 토큰을 모아 등장 빈도 계산\n",
    "all_tokens = [token for tokens in train_data['tokenized'] for token in tokens]\n",
    "counter = Counter(all_tokens)\n",
    "# 가장 많이 등장하는 단어 vocab_size 개수만큼 남기기\n",
    "counter = counter.most_common(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a31618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary 파일로 저장\n",
    "word_to_index_path = './vocab.txt'\n",
    "\n",
    "def save_word_index(counter):\n",
    "    with open(word_to_index_path, 'w') as f:\n",
    "        for idx, (word, _) in enumerate(counter):\n",
    "            line = f\"{word}: {idx}\\n\"\n",
    "            f.write(line)\n",
    "            \n",
    "#save_word_index(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf59b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {token: idx for idx, token in enumerate(special_tokens[:2])}\n",
    "word_to_index.update({token: idx + len(special_tokens[:2]) for idx, (token, _) in enumerate(counter)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70692255",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80fada3",
   "metadata": {},
   "source": [
    "#### 정수인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9994b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'tokenized' 열의 데이터를 정수 인코딩\n",
    "def encode_tokens(tokens, word_to_index):\n",
    "    unk_index = word_to_index['<unk>']\n",
    "    return [word_to_index.get(token, unk_index) for token in tokens]\n",
    "\n",
    "train_data['encoded'] = train_data['tokenized'].apply(lambda x: encode_tokens(x, word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763cfecd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e82570",
   "metadata": {
    "id": "e970c394"
   },
   "source": [
    "## 전처리 후 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9681d4b",
   "metadata": {},
   "source": [
    "#### 토큰화된 데이터 길이 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aba2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'encoded' 열의 각 리스트의 길이 계산\n",
    "lengths = train_data['encoded'].apply(len)\n",
    "\n",
    "print(f\"최대 길이: {lengths.max()}\")\n",
    "print(f\"최소 길이: {lengths.min()}\")\n",
    "print(f\"평균 길이: {lengths.mean()}\")\n",
    "print(f\"표준편차: {lengths.std()}\")\n",
    "\n",
    "# 길이 분포 시각화\n",
    "plt.hist(lengths, bins=50, color='skyblue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471c10d",
   "metadata": {},
   "source": [
    "#### 패딩 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e77246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화 최대 길이 150으로 설정\n",
    "MAX_LENGTH = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9508ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(train_data['encoded'], maxlen=MAX_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e597f",
   "metadata": {
    "id": "0b636ead"
   },
   "source": [
    "## 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data['type']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(X_train))\n",
    "print('훈련 레이블의 개수 :', len(y_train))\n",
    "print('검증 데이터의 개수 :', len(X_val))\n",
    "print('검증 레이블의 개수 :', len(y_val))\n",
    "print('테스트 데이터의 개수 :', len(X_test))\n",
    "print('테스트 레이블의 개수 :', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b70dd",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8338a18",
   "metadata": {},
   "source": [
    "```!pip install wandb==0.16.0 -qq```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e35d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.login(key = '809618c39f10bc0019fd6fd710cb28c698c30197')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c676a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"name\": \"sweep_test_nlp\",\n",
    "    \"metric\": {\"name\": \"val_loss\", \"goal\": \"minimize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\" : {\n",
    "            \"min\" : 0.001,\n",
    "            \"max\" : 0.1\n",
    "            },\n",
    "        \"epoch\" : {\n",
    "            \"distribution\" : \"int_uniform\",\n",
    "            \"min\" : 5,\n",
    "            \"max\" : 10\n",
    "            },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [16, 32, 64]\n",
    "            },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"adam\", \"sgd\", \"rmsprop\"]\n",
    "            }           \n",
    "        }\n",
    "    }\n",
    "\n",
    "default_config = {\n",
    "        \"vocab\" : VOCAB_SIZE,\n",
    "        \"embeddings\" : 128,\n",
    "        \"units_128\" : 128,\n",
    "        \"units_256\" : 256,\n",
    "        \"units_512\" : 512,\n",
    "        \"units_1024\" : 1024,\n",
    "        \"units_2048\" : 2048,\n",
    "        \"kernel_3\" : 3,\n",
    "        \"kernel_5\" : 5,\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"class_num\" : 5,\n",
    "        \"loss\" : \"sparse_categorical_crossentropy\",\n",
    "        \"metrics\" : [\"accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_baseline(config):\n",
    "    model=keras.models.Sequential()\n",
    "    model.add(keras.layers.Embedding(config.vocab, config.embeddings))\n",
    "    model.add(keras.layers.GRU(units = config.units_256, return_sequences = True))\n",
    "    model.add(keras.layers.GRU(units = config.units_512))\n",
    "    model.add(keras.layers.Dense(config.units_1024, activation='relu'))\n",
    "    model.add(keras.layers.Dense(config.class_num, activation='softmax'))  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7166b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_1DCNN(config):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Embedding(config.vocab, config.embeddings))\n",
    "    model.add(keras.layers.Conv1D(config.embeddings, config.kernel_5, activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=4))\n",
    "    model.add(keras.layers.GlobalMaxPooling1D())\n",
    "    model.add(keras.layers.Dense(config.units_128, activation='relu'))\n",
    "    model.add(keras.layers.Dense(config.class_num, activation='softmax')) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f52c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_1DCNN_GRU(config):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Embedding(config.vocab, config.embeddings))\n",
    "    model.add(keras.layers.SpatialDropout1D(config.dropout_rate))\n",
    "    model.add(keras.layers.Conv1D(config.embeddings, config.kernel_5, activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=4))\n",
    "    model.add(keras.layers.GRU(config.units_128, dropout=config.dropout_rate, recurrent_dropout=config.dropout_rate))\n",
    "    model.add(keras.layers.Dense(config.class_num, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch이 모두 끝나면 confusion matrix를 wandb에 그려주는 콜백 생성\n",
    "class ConfusionMatrixCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, X_test, y_test, class_num):\n",
    "        super().__init__()\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.class_num = class_num\n",
    "    # confusoin matrix heatmap으로 시각화\n",
    "    def plot_confusion_matrix(self, cm, classes, title):\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes, annot_kws={'size': 15})\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title(title)\n",
    "        return plt\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch == self.params['epochs'] - 1:  # 마지막 epoch에서만 계산\n",
    "            pred_test = self.model.predict(self.X_test).argmax(axis=1)\n",
    "            # numpy arrays로 변환\n",
    "            self.y_test = np.array(self.y_test)\n",
    "            pred_test = np.array(pred_test)\n",
    "            \n",
    "            # 클래스에서 두 개씩 뽑아내어 총 10개의 쌍 만들기\n",
    "            class_pairs = list(combinations(range(self.class_num), 2))\n",
    "            for class_a, class_b in class_pairs:\n",
    "                indices = np.where((self.y_test == class_a) | (self.y_test == class_b))[0]\n",
    "                y_test_subset = self.y_test[indices]\n",
    "                pred_test_subset = pred_test[indices]\n",
    "\n",
    "                # 클래스를 binary label로 매핑\n",
    "                y_test_subset = np.where(y_test_subset == class_a, 0, 1)\n",
    "                pred_test_subset = np.where(pred_test_subset == class_a, 0, 1)\n",
    "                \n",
    "                # confusion matrix 생성\n",
    "                cm = confusion_matrix(y_test_subset, pred_test_subset)\n",
    "                cm_plot = self.plot_confusion_matrix(cm, [\"class 0\", \"class 1\"], f\"Confusion Matrix: class {class_a} vs class {class_b}\")\n",
    "                # wandb 내에 confusion matrix 그리기\n",
    "                cm_image = wandb.Image(cm_plot)\n",
    "                wandb.log({f\"Confusion Matrix for class {class_a} vs class {class_b}\": cm_image})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수 정의\n",
    "# ConfusionMatrixCallback에서 테스트 데이터셋을 활용하므로 인자로 넣어줌\n",
    "def train(default_config, X_test, y_test):\n",
    "\n",
    "    wandb.init(config = default_config)\n",
    "    config = wandb.config\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    # Model\n",
    "    model = build_model_baseline(config)\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer = config.optimizer,\n",
    "                  loss = config.loss,\n",
    "                  metrics = config.metrics)\n",
    "    \n",
    "    # confunsion matrix 그리는 콜백\n",
    "    cm_callback = ConfusionMatrixCallback(X_test, y_test, config.class_num)\n",
    "    \n",
    "    # 학습\n",
    "    history = model.fit(X_train, y_train,\n",
    "              epochs = config.epoch,\n",
    "              batch_size = config.batch_size,\n",
    "              validation_data = (X_val, y_val),\n",
    "              callbacks=[wandb.keras.WandbCallback(), cm_callback])\n",
    "    \n",
    "    # test dataset으로 f1 score 계산\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "    pred_test = model.predict(X_test).argmax(axis=1)\n",
    "    f1_score_res = f1_score(y_test, pred_test, average='micro')\n",
    "\n",
    "    # wandb에 log 추가\n",
    "    wandb.log({\n",
    "        \"Test Accuracy Rate\": test_accuracy,\n",
    "        \"Test F1 Score\": f1_score_res,\n",
    "        \"Test Error Rate\": 1 - test_accuracy\n",
    "    })\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3e7a5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train()에 인자가 있으므로 wrapper function 정의\n",
    "def sweep_train():\n",
    "    train(default_config=default_config, X_test=X_test, y_test=y_test)\n",
    "\n",
    "# 팀프로젝트 내에서 sweep 실행\n",
    "sweep_id = wandb.sweep(sweep_config,\n",
    "                       entity = 'aiffel_minions',\n",
    "                       project = 'DLthon_baseline')\n",
    "\n",
    "wandb.agent(sweep_id,\n",
    "            function=sweep_train,\n",
    "            count=3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
